{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import os.path\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import openpyxl\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import SessionNotCreatedException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoSuchWindowException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동적 웹 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 맛집 블로그 포스트 100개 크롤링 - 제목, url, 장소명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPostDate(post_title):        \n",
    "    # 검색된 포스트의 날짜 구하기 - Naver API 활용\n",
    "    client_id = \"XVCwmN8OWSwXTVWc2_uQ\"\n",
    "    client_secret = \"45zos8jb_F\"\n",
    "\n",
    "    encText = urllib.parse.quote(str(post_title))\n",
    "\n",
    "    url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText # JSON 결과\n",
    "    display_num = \"1\" #출력할 갯수 입력받기\n",
    "    url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText +\"&display=\"+display_num# json 결과\n",
    "    # url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # XML 결과\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if(rescode==200):\n",
    "        response_body = response.read().decode('utf-8')\n",
    "        # print(response_body)\n",
    "        # json 파일로 변경\n",
    "        with open(\"response.json\", \"w\") as json_file:\n",
    "            json.dump(response_body, json_file)\n",
    "        res_json = json.dumps(response_body, indent=4, sort_keys=True)\n",
    "        with open(\"response.json\", \"r\") as response_json:\n",
    "            res_python = json.load(response_json)\n",
    "\n",
    "        #items key 값만 뽑기\n",
    "        my_dict = ast.literal_eval(res_python)\n",
    "        list_str = str(my_dict[\"items\"])[1:-1]\n",
    "        # json 파일로 변경\n",
    "        with open(\"items.json\", \"w\") as json_file:\n",
    "            json.dump(list_str, json_file)\n",
    "        res_json = json.dumps(list_str, indent=4, sort_keys=True)\n",
    "        with open(\"items.json\", \"r\") as response_json:\n",
    "            res_python = json.load(response_json)\n",
    "        my_dict = ast.literal_eval(res_python)\n",
    "        postdate = my_dict[\"postdate\"]\n",
    "        # print(\"postdate : {}\".format(postdate))\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "    return postdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetResInfo(raw_url):\n",
    "\n",
    "    temp = []\n",
    "    tags = []\n",
    "\n",
    "    #파싱 가능한 실제 viewer url 얻기\n",
    "    # print(raw_url)\n",
    "    url = \"view-source:{}\".format(raw_url)\n",
    "    # print(url)\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        #크롬 드라이버 설치 링크 : https:storage.googleapis.com/chrome-for-testing-public/127.0.6533.72/win64/chromedriver-win64.zip\n",
    "        #압축 해제 후, driver.exe는 실행파일과 같은 위치 혹은 하위에 존재해야 함\n",
    "        #사내망에선 드라이버의 물리적 위치 괄호 안에 지정해줘야 함\n",
    "    except SessionNotCreatedException as e:\n",
    "        print(\"SessionNotCreatedExceptions이 발생했습니다:\", str(e))\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    a = driver.find_element(By.CSS_SELECTOR, 'body > table > tbody > tr:nth-child(76) > td.line-content > a').text\n",
    "    # print(a)\n",
    "    real_url = \"https://blog.naver.com{}\".format(a)\n",
    "    \n",
    "    #파싱 가능한 url로 재진입\n",
    "    driver.get(real_url)\n",
    "    time.sleep(3)\n",
    "    print(real_url)\n",
    "\n",
    "    #식당명, 식당주소 얻기\n",
    "    page = urllib.request.urlopen(real_url)\n",
    "    soup = BeautifulSoup(page,'html.parser')\n",
    "    try:\n",
    "        # for tag in soup.find_all(class_='se-map-title'):\n",
    "        for tag in soup.find(class_='se-map-title'):\n",
    "            restaurant_name = tag.get_text()\n",
    "            print(restaurant_name)\n",
    "        # for tag in soup.find_all(class_='se-map-address'):\n",
    "        for tag in soup.find(class_='se-map-address'):\n",
    "            restaurant_adress = tag.get_text()\n",
    "            print(restaurant_adress)\n",
    "    except TypeError as e:\n",
    "        restaurant_name = \"\"\n",
    "        restaurant_adress = \"\"\n",
    "        print(\"본문내 지도 위젯이 없습니다.\")\n",
    "\n",
    "    # 해시태그 확인용 url 재진입\n",
    "    # 해시태그 얻기    \n",
    "    # b = driver.find_element(By.CSS_SELECTOR, '#post_footer_contents').text\n",
    "    # str(b).split('\\n')\n",
    "    # temp = list(b.split('\\n'))\n",
    "    # tags = temp[4:-1]\n",
    "    # print(tags)\n",
    "\n",
    "    #해시태그 얻기_2024.08.26 수정\n",
    "    b = driver.find_element(By.CSS_SELECTOR, '#post_footer_contents').text\n",
    "    temp = list(str(b).split('\\n'))\n",
    "    tags = temp[1:]\n",
    "    if '영리적 사용 불가' in tags:\n",
    "        tags.remove('영리적 사용 불가')\n",
    "    if '내용 변경 불가' in tags:\n",
    "        tags.remove('내용 변경 불가')\n",
    "    if '태그' in tags:\n",
    "        tags.remove('태그')\n",
    "    tags_ = ', '.join(tags)\n",
    "    print(tags)\n",
    "    print(tags_)\n",
    "\n",
    "    time.sleep(3)\n",
    "    driver.close()\n",
    "    \n",
    "    return str(restaurant_name), str(restaurant_adress), str(tags_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetResInfo_old(url):\n",
    "    print(url)\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        #크롬 드라이버 설치 링크 : https:storage.googleapis.com/chrome-for-testing-public/127.0.6533.72/win64/chromedriver-win64.zip\n",
    "        #압축 해제 후, driver.exe는 실행파일과 같은 위치 혹은 하위에 존재해야 함\n",
    "        #사내망에선 드라이버의 물리적 위치 괄호 안에 지정해줘야 함\n",
    "    except SessionNotCreatedException as e:\n",
    "        print(\"SessionNotCreatedExceptions이 발생했습니다:\", str(e))\n",
    "        \n",
    "    #블로그 링크 하나씩 불러서 크롤링\n",
    "    tags = []\n",
    "    contents = []\n",
    "    restaurant_name = []\n",
    "    restaurant_address = []\n",
    "    \n",
    "    #블로그 링크 하나씩 불러오기\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    #블로그 안 본문이 있는 iframe에 접근하기\n",
    "    driver.switch_to.frame(\"mainFrame\")\n",
    "\n",
    "    #본문 전체\n",
    "    try:\n",
    "        a = driver.find_element(By.CSS_SELECTOR,'div.se-main-container').text\n",
    "        contents.append(a)\n",
    "    # NoSuchElement 오류시 예외처리(구버전 블로그에 적용)\n",
    "    except NoSuchElementException:\n",
    "        a = driver.find_element(By.CSS_SELECTOR,'div#content-area').text\n",
    "        contents.append(a)\n",
    "    #print(본문: \\n', a)\n",
    "\n",
    "    #해시태그\n",
    "    # try:\n",
    "    #     t = driver.find_element(By.CSS_SELECTOR,'div.post-btn post_btn2')\n",
    "    #     tags.append(t)\n",
    "    # except NoSuchElementException:\n",
    "    #     print(\"해시태그를 찾을 수 없습니다.\")\n",
    "\n",
    "    # driver.quit() #창닫기\n",
    "    print(\"<<본문 크롤링이 완료되었습니다.>>\")\n",
    "    text = contents[0]\n",
    "    contents_list = text.split(\"\\n\")\n",
    "    print(contents_list)\n",
    "\n",
    "    print(\"<<본문의 장소 정보입니다.>>\")\n",
    "    try:\n",
    "        found = contents_list.index('© NAVER Corp.') ## 네이버 지도 전체가 위젯으로 넣어진 경우\n",
    "        restaurant_name.append(contents_list[found+1])\n",
    "        restaurant_address.append(contents_list[found+2])\n",
    "        print(restaurant_name)\n",
    "        print(restaurant_address)\n",
    "    except ValueError:\n",
    "        try : \n",
    "            found = contents_list.index('예약')\n",
    "            restaurant_name.append(contents_list[found-2]) ##네이버 지도 일부가 위젯으로 넣어지거나, \n",
    "            restaurant_address.append(contents_list[found-1])\n",
    "            print(restaurant_name)\n",
    "            print(restaurant_address)\n",
    "        except ValueError:\n",
    "            try : \n",
    "                found = contents_list.index('주소')\n",
    "                restaurant_name.append(contents_list[found-1]) ##지도가 본문에 첨부되지 않은 경우\n",
    "                restaurant_address.append(contents_list[found+1])\n",
    "                print(restaurant_name)\n",
    "                print(restaurant_address)\n",
    "            except ValueError:\n",
    "                print(\"장소 정보를 찾을 수 없습니다.\")\n",
    "                restaurant_name.append(\" \")\n",
    "                restaurant_address.append(\" \")\n",
    "                print(restaurant_name)\n",
    "                print(restaurant_address)\n",
    "        \n",
    "    try:\n",
    "        tag = [s for s in contents_list if \"#\" in s] \n",
    "        print(tag)\n",
    "        tags.append(tag)\n",
    "    except IndexError:\n",
    "            print(\"해시태그를 찾을 수 없습니다.\")\n",
    "\n",
    "    return restaurant_name[0], restaurant_address[0], str(tags)\n",
    "    driver.quit() #창닫기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNaverBlog(url, crawler_date, crawler_time):\n",
    "    App = \"Naver Blog\"\n",
    "    data = []\n",
    "\n",
    "    #크롬 드라이버 연결\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        #크롬 드라이버 설치 링크 : https:storage.googleapis.com/chrome-for-testing-public/127.0.6533.72/win64/chromedriver-win64.zip\n",
    "        #압축 해제 후, driver.exe는 실행파일과 같은 위치 혹은 하위에 존재해야 함\n",
    "        #사내망에선 드라이버의 물리적 위치 괄호 안에 지정해줘야 함\n",
    "    except SessionNotCreatedException as e:\n",
    "        print(\"SessionNotCreatedExceptions이 발생했습니다:\", str(e))\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # post_titles = driver.find_elements(By.CLASS_NAME,'title_post')\n",
    "\n",
    "    #크롤링 시작\n",
    "    print(\"-----네이버 블로그 - 맛집 포스트-----\")\n",
    "    for page in range(1,2): ##1~n-1 페이지까지. 5분 소요\n",
    "        driver.find_element(By.XPATH, '/html/body/ui-view/div/main/div[1]/div/section/div[2]/div[11]/span[{}]/a'.format(page)).click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        #포스트 제목\n",
    "        post_titles = driver.find_elements(By.CLASS_NAME,'title_post')\n",
    "\n",
    "        #포스트 url, 식당명, 식당주소\n",
    "        post_urls = driver.find_elements(By.CLASS_NAME,'desc_inner')\n",
    "        urls = []\n",
    "        # post_dates = []\n",
    "        name = []\n",
    "        address = []\n",
    "        tags = []\n",
    "\n",
    "        for i in post_urls :\n",
    "            href = i.get_attribute('href')\n",
    "            # print(href)\n",
    "            urls.append(href)\n",
    "        # print(urls)\n",
    "        \n",
    "        for url in urls :\n",
    "            res_name, res_address, tag = GetResInfo(url)\n",
    "            name.append(res_name)\n",
    "            address.append(res_address)\n",
    "            tags.append(tag)\n",
    "\n",
    "        # for post_title in post_titles:\n",
    "        #     print(post_title)\n",
    "        #     post_date = GetPostDate(post_title)\n",
    "        #     post_dates.append(post_date)\n",
    "            \n",
    "\n",
    "        for idx, post_title in enumerate(post_titles[9:]): ## Top 3 제거(제목너무 짧음), 공백 행 제거\n",
    "            title = post_title.text\n",
    "            line_data = []\n",
    "            line_data.append(App)\n",
    "            line_data.append(GetPostDate(title))\n",
    "            line_data.append(title)\n",
    "            line_data.append(urls[idx])\n",
    "            line_data.append(name[idx])\n",
    "            line_data.append(address[idx])\n",
    "            line_data.append(tags[idx])\n",
    "            line_data.append(crawler_time)\n",
    "            data.append(line_data)\n",
    "        \n",
    "    #데이터 프레임 생성\n",
    "    df = pd.DataFrame(data, columns=['App', 'Post Date', 'Post Title', 'URL', 'Restaurant Name', 'Restaurant Address', 'Tag', 'Crawler Time'])\n",
    "    df.index = df.index+1\n",
    "    df\n",
    "\n",
    "    #엑셀로 저장\n",
    "    df.to_excel('Results/FD_Crawling_Naver_Blog_{}.xlsx'.format(crawler_date), index=False)\n",
    "    time.sleep(3)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 공정위 가맹사업정보제공시스템 정보공개서 크롤링 - 100개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBusinessInfoSystem (url, crawler_date, crawler_time):\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        #크롬 드라이버 설치 링크 : https:storage.googleapis.com/chrome-for-testing-public/127.0.6533.72/win64/chromedriver-win64.zip\n",
    "        #압축 해제 후, driver.exe는 실행파일과 같은 위치 혹은 하위에 존재해야 함\n",
    "        #사내망에선 드라이버의 물리적 위치 괄호 안에 지정해줘야 함\n",
    "    except SessionNotCreatedException as e:\n",
    "        print(\"SessionNotCreatedExceptions이 발생했습니다:\", str(e))\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    #외식 업종, 100개씩 보기, 적용 조회\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div[2]/div[2]/form/div[1]/fieldset/select[2]/option[2]').click()\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div[2]/div[2]/form/div[1]/fieldset/input[2]').click()\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div[2]/div[2]/form/div[2]/div/select/option[4]').click()\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div[2]/div[2]/form/div[2]/div/a').click() #적용 클릭\n",
    "    time.sleep(3)\n",
    "    table = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div[2]/div[2]/form/table')\n",
    "    tbody = table.find_element(By.TAG_NAME, 'tbody')\n",
    "\n",
    "    i = 1\n",
    "    data = []\n",
    "    line_data = []\n",
    "    \n",
    "    wb = openpyxl.Workbook()\n",
    "    sheet = wb.active\n",
    "    sheet.append([\"No.\", \"Name\", \"Business\", \"Representative\",\"Registration_Number\", \"Init_Date\",\"Type\"])\n",
    "    # 번호, 상호, 영업표지, 대표자, 등록번호, 최초등록일, 업종\n",
    "\n",
    "    rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "    #테이블 파싱, 컬럼명 지정\n",
    "    for index, value in enumerate(rows):\n",
    "        Name = value.find_elements(By.TAG_NAME,\"td\")[1].text\n",
    "        Business = value.find_elements(By.TAG_NAME,\"td\")[2].text\n",
    "        Representative = value.find_elements(By.TAG_NAME,\"td\")[3].text\n",
    "        Registration_Number = value.find_elements(By.TAG_NAME,\"td\")[4].text\n",
    "        Init_Date = value.find_elements(By.TAG_NAME,\"td\")[5].text\n",
    "        Type = value.find_elements(By.TAG_NAME,\"td\")[6].text\n",
    "\n",
    "        sheet.append([i, Name, Business, Representative, Registration_Number, Init_Date, Type])\n",
    "        i +=1\n",
    "\n",
    "    for row in sheet.iter_rows(max_col=7, values_only=True):\n",
    "        print(row)\n",
    "        line_data.append(row)\n",
    "        data.append(line_data)\n",
    "\n",
    "    #saving to excel\n",
    "    # 정렬 전 저장\n",
    "    wb.save('./results/FD_Crawling_Business_Info_System_{}.xlsx'.format(crawler_date))\n",
    "\n",
    "    # 정렬 후 저장\n",
    "    xl = pd.ExcelFile('./results/FD_Crawling_Business_Info_System_{}.xlsx'.format(crawler_date))\n",
    "    df = xl.parse(\"Sheet\")\n",
    "    df = df.sort_values(by=\"Init_Date\", ascending=False)\n",
    "    df.to_excel('./results/FD_Crawling_Business_Info_System_{}.xlsx'.format(crawler_date), sheet_name=\"가맹사업정보\", columns=[\"No.\", \"Name\", \"Business\", \"Representative\",\"Registration_Number\", \"Init_Date\",\"Type\"],index=False)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_excel(file_path, total_file, arch_folder):\n",
    "\n",
    "    if os.path.isfile(total_file) :\n",
    "        print(\"기준 엑셀 파일에 병합합니다...\")\n",
    "        df1 = pd.read_excel(file_path)\n",
    "        df2 = pd.read_excel(total_file)\n",
    "        \n",
    "        df_total = pd.concat([df1,df2], ignore_index=False)\n",
    "        df_total.drop_duplicates(subset=None, keep='first',inplace=False, ignore_index=False)\n",
    "        df_total = df_total.drop('No.',axis=1)\n",
    "\n",
    "    else:\n",
    "        print(\"파일이 존재하지 않습니다.\")\n",
    "    \n",
    "    # print(df_total)\n",
    "    df_total.to_excel('./results/FD_Crawling_Business_Info_System_Total.xlsx', index=False)\n",
    "    print(\"병합을 완료하고 해당 파일을 보관합니다...\")\n",
    "    \n",
    "    import shutil\n",
    "    # 이미 존재하면 덮어 쓰는 조건문 만들기\n",
    "    shutil.move(file_path, arch_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # 네이버 맛집 블로그 콘텐츠 크롤링\n",
    "    crawler_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    crawler_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "    crawler_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "    print(\"Naver crawler_date : {}\".format(crawler_time))\n",
    "\n",
    "    naver_url = \"https://section.blog.naver.com/ThemePost.naver?directoryNo=29&activeDirectorySeq=3&currentPage=1\" #네이버 블로그 맛집 카테고리 메인 화면\n",
    "    GetNaverBlog(naver_url, crawler_date, crawler_time)\n",
    "\n",
    "    # 가맹사업정보시스템 크롤링\n",
    "    crawler_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    crawler_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "    crawler_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "    print(\"가맹사업정보시스템 crawler_date : {}\".format(crawler_time))\n",
    "\n",
    "    business_info_system_url = \"https://franchise.ftc.go.kr/mnu/00013/program/userRqst/list.do\"\n",
    "    GetBusinessInfoSystem(business_info_system_url, crawler_date, crawler_time)\n",
    "\n",
    "    file_list = glob.glob('./results/FD_Crawling_Business_Info_System_2*.xlsx')\n",
    "    total_file = './results/FD_Crawling_Business_Info_System_Total.xlsx'\n",
    "    arch_folder = './results/FD_Crawling_Business_Info_System_Archaive'\n",
    "\n",
    "    for file_path in file_list:\n",
    "        print(file_path)\n",
    "        print(total_file)\n",
    "        merge_excel(file_path, total_file,arch_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results\\FD_Crawling_Business_Info_System_20240827.xlsx\n",
      "./results/FD_Crawling_Business_Info_System_Total.xlsx\n",
      "기준 엑셀 파일에 병합합니다...\n",
      "병합을 완료하고 해당 파일을 보관합니다...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
